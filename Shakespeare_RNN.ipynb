{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7ujZrltOw1Q"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "34deyUZIO3H6",
    "outputId": "5f7f2529-22e7-4e2f-d174-aacc83cc3ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "RQlpBfK1Peji",
    "outputId": "56572b12-480f-475c-c56e-bf3b6949be75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of no of characters: 1115394 \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(file= path_to_file).read()\n",
    "print('Length of no of characters:', len(text),'\\n')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K4bsFgi2PqxU",
    "outputId": "e4653a01-b6ee-4c4e-abf3-8310f7653f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 65 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique characters\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print('Length of vocab:', len(vocab),'\\n')\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YTv727q5W296",
    "outputId": "e6c1bb23-4302-4680-f617-b8df54e874cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = {j:i for i , j in enumerate(vocab)}\n",
    "\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "5-j0V1BCZIY4",
    "outputId": "821ec787-47ec-4b98-e66b-73b022c5148f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n"
     ]
    }
   ],
   "source": [
    "# change all characters to integers by the mapping of numbers of char_to_idx\n",
    "print(text[:100], '\\n')\n",
    "\n",
    "text_to_int = np.array([char_to_idx[i] for i in text])\n",
    "print(text_to_int[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBVTmnN8gAqO"
   },
   "source": [
    "**The prediction task**\n",
    "\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
    "\n",
    "**Create training examples and targets**\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the [tf.data.Dataset.from_tensor_slices](https://) function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "owqYbTnXdSAo",
    "outputId": "f6b6262a-321c-4320-aa5c-e5a96131c690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_epoch: 11043 \n",
      "\n",
      "idx_to_char:\n",
      " ['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z'] \n",
      "\n",
      "First five elements:\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# This length is used to slice the text\n",
    "seq_length= 100\n",
    "\n",
    "examples_per_epoch = len(text) // (seq_length+1)\n",
    "print('examples_per_epoch:', examples_per_epoch, '\\n')\n",
    "\n",
    "# Create examples slicing all of text individually to pass each character.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(tensors =  text_to_int)\n",
    "\n",
    "# idx_to_char means index to char in np.array\n",
    "idx_to_char = np.array(vocab)\n",
    "print('idx_to_char:\\n', idx_to_char, '\\n')\n",
    "print('First five elements:')\n",
    "for i in char_dataset.take(count= 5):\n",
    "  print(idx_to_char[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "683bONJGtcbm"
   },
   "source": [
    "# The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
    "\n",
    "drop_remainder= False\n",
    "\n",
    "i/p --> dataset = tf.data.Dataset.range(8) dataset = dataset.batch(3)\n",
    "\n",
    "o/p --> list(dataset.as_numpy_iterator()) [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
    " \n",
    "drop_remainder= True \n",
    "(means truncate the last if the length of tensor doesnt match with existing tensors)\n",
    "\n",
    "i/p --> dataset = tf.data.Dataset.range(8) dataset = dataset.batch(3, drop_remainder=True) \n",
    "\n",
    "o/p --> list(dataset.as_numpy_iterator()) [array([0, 1, 2]), array([3, 4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "GJUdJVPpmTlY",
    "outputId": "a7b159a4-5368-4c23-cf06-245427605f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each sequence: 101\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You  \n",
      "\n",
      "Length of each sequence: 101\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# seq_length + 1 is shown in below 'dataset'\n",
    "# if seq_length= 4 and seq_length + 1 --> 'hello' = 'hell' and 'ello'\n",
    "sequences = char_dataset.batch(batch_size= seq_length+1, drop_remainder= True)\n",
    "\n",
    "for i in sequences.take(count= 2):\n",
    "  print('Length of each sequence:',len(i))\n",
    "  print(\"\".join(idx_to_char[i]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4zVA65ttW48"
   },
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "rIispR7esG1s",
    "outputId": "3e42c9ab-7e05-4501-9bc3-9319abaaf1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input sequence: 100 \n",
      "\n",
      "Input_text: \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "Length of target sequence: 100 \n",
      "\n",
      "Target_text: \n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_chunks(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text=  chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(map_func= split_chunks)\n",
    "\n",
    "# We print and check\n",
    "for i, j in dataset.take(count= 1):\n",
    "  print('Length of input sequence:', len(i), '\\n')\n",
    "  print('Input_text:', '\\n', \"\".join(idx_to_char[i]), '\\n')\n",
    "  print('Length of target sequence:', len(j), '\\n')\n",
    "  print('Target_text:', '\\n', \"\".join(idx_to_char[j]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMpWbA4MwHv2"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "hk82xbF6vbdY",
    "outputId": "9a156b22-a3d2-423c-f748-ca1ffa235ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Input:  F, 18 \n",
      "Output: i, 47  \n",
      "\n",
      "Step: 1\n",
      "Input:  i, 47 \n",
      "Output: r, 56  \n",
      "\n",
      "Step: 2\n",
      "Input:  r, 56 \n",
      "Output: s, 57  \n",
      "\n",
      "Step: 3\n",
      "Input:  s, 57 \n",
      "Output: t, 58  \n",
      "\n",
      "Step: 4\n",
      "Input:  t, 58 \n",
      "Output:  , 1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, (n, o) in enumerate(zip(i[:5], j[:5])):\n",
    "  print('Step:', m)\n",
    "  print('Input:  {}, {} '.format(idx_to_char[n], n))\n",
    "  print('Output: {}, {} '.format(idx_to_char[o], o),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jg_nblE1UxG"
   },
   "source": [
    "**Create training batches.**\n",
    "\n",
    "We used tf.data to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches.\n",
    "\n",
    "**Randomly shuffles the elements of this dataset.**\n",
    "\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lQu03OHqyt76",
    "outputId": "74529f0d-ad21-461d-a79f-c7baa51fb0f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "batch_size= 64\n",
    "\n",
    "steps_per_epoch = examples_per_epoch//batch_size\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "buffer_size= 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size= buffer_size)\n",
    "dataset = dataset.batch(batch_size= batch_size, drop_remainder= True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAC3FJEj2WHU"
   },
   "source": [
    "**Build The Model**\n",
    "\n",
    "Use tf.keras.Sequential to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "    tf.keras.layers.Embedding: The input layer. \n",
    "    A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "    \n",
    "    tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
    "    tf.keras.layers.Dense: The output layer, with vocab_size outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_udKu9S2OEA"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "B6b6qzTe8d0O",
    "outputId": "fab5756f-a06c-4efd-b2de-30d53cfa93bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This is identical to the following: \n",
    "# model = tf.keras.Sequential() \n",
    "# model.add(tf.keras.layers.Dense(8, input_dim=16)) and so on...\n",
    "\n",
    "def model_build(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential(layers=[tf.keras.layers.Embedding(input_dim= vocab_size, output_dim= embedding_dim, \n",
    "                                                                batch_input_shape= [batch_size, None]),\n",
    "                                    tf.keras.layers.LSTM(units= rnn_units, return_sequences= True, stateful= True,\n",
    "                                                         recurrent_initializer= 'glorot_uniform'),\n",
    "                                    tf.keras.layers.Dense(units= vocab_size,) #  activation= 'softmax' not used and logits are used\n",
    "                                    ] )\n",
    "  return model\n",
    "\n",
    "model = model_build( vocab_size = vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size= batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIgwCVW5_Sd6"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. \n",
    "This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UuPvCfWA8dXG",
    "outputId": "367d13e8-e43e-4fff-ec93-b1af1dcf4494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 64, sequence length: 100, vocab_size: 65 \n"
     ]
    }
   ],
   "source": [
    "for ip, op in dataset.take(count= 1):\n",
    "  # pass input into model and we get 1st batch shape as\n",
    "  example_batch = model(ip)\n",
    "  # https://stackoverflow.com/a/15181942/10219869\n",
    "  print('batch size: {0}, sequence length: {1}, vocab_size: {2} '.format(*example_batch.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "px9pPJYzLHee",
    "outputId": "2416d55d-9551-47b8-cadd-b89859e0f93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([36, 27, 50,  8, 22, 11, 52, 41, 21, 53, 56, 44, 50, 29, 31, 52, 12,\n",
       "       29,  8, 45, 16, 50, 31,  0, 29, 54,  7, 51,  8, 40,  7, 64,  4, 53,\n",
       "       31, 29, 49, 64, 54, 29, 27, 38, 16, 52, 61, 13,  4,  9,  5,  2, 32,\n",
       "        7,  8, 44,  2,  3, 30, 38, 20, 38, 20, 11, 40, 24, 57, 39, 58, 51,\n",
       "       13, 36, 38, 37, 60, 62, 63, 12, 43, 23,  6, 56, 56, 19, 48, 28, 32,\n",
       "       48, 45, 23, 57, 16, 14, 42, 26, 64,  0, 63, 41, 52, 63, 20])>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we try a sample prediction\n",
    "# example_batch[0] means first row of 100 out of 64 rows.\n",
    "# we obtain indices\n",
    "\n",
    "indices = tf.random.categorical(example_batch[0], num_samples= 1)\n",
    "# https://stackoverflow.com/a/58843005/10219869 to understand squeeze\n",
    "indices = tf.squeeze(indices, axis= -1)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "n2obLZMAPdzm",
    "outputId": "9ead5a60-687e-4d9c-e0e1-330eccaf3a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "ess than I was born to:\n",
      "A man at least, for less I should not be;\n",
      "And men may talk of kings, and why \n",
      "\n",
      "Next char predictions:\n",
      "XOl.J;ncIorflQSn?Q.gDlS\n",
      "Qp-m.b-z&oSQkzpQOZDnwA&3'!T-.f!$RZHZH;bLsatmAXZYvxy?eK,rrGjPTjgKsDBdNz\n",
      "ycnyH\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted by this untrained model:\n",
    "\n",
    "print('Input:')\n",
    "# without join, the output gets separated\n",
    "print(''.join(idx_to_char[ip[0]]), '\\n')\n",
    "print('Next char predictions:')\n",
    "print(''.join(idx_to_char[indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7H1NRddg_3Xo",
    "outputId": "6196fb59-123b-4b8f-b486-4b4852d1881d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.173674"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_loss= tf.keras.losses.sparse_categorical_crossentropy(y_true= op, y_pred= example_batch, from_logits= True )\n",
    "\n",
    "example_loss.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEDdXTirk8TW"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint=tf.keras.callbacks.ModelCheckpoint( filepath= '/content/rnn.h5', save_weights_only=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsfByEpGLKgs"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer= 'rmsprop', loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "O84xAsIlB0sI",
    "outputId": "d03e9648-aa81-462e-9c2e-7d237ca8dbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 2.6894\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 9s 54ms/step - loss: 1.9237\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.6474\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.5084\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.4262\n"
     ]
    }
   ],
   "source": [
    "epoch= 5\n",
    "\n",
    "history = model.fit(dataset.repeat(), epochs= epoch, steps_per_epoch= steps_per_epoch, callbacks= checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "eQpbiFihbmmw",
    "outputId": "5fdb53ee-980e-461c-b8c3-59401c6b2812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_build(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights('/content/rnn.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64BJXvZoMsak"
   },
   "source": [
    "**The prediction loop**\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "    It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "    Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "    Then, use a categorical distribution to calculate the index of the predicted character. \n",
    "    Use this predicted character as our next input to the model.\n",
    "\n",
    "    The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. \n",
    "    After predicting the next character, the modified RNN states are again fed back into the model, \n",
    "    which is how it learns as it gets more context from the previously predicted characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "id": "CU9pdi7DYv58",
    "outputId": "3de43757-0a25-4434-9101-618d950ce4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 27, 25, 17, 27, 10, 1]\n",
      "tf.Tensor([[30 27 25 17 27 10  1]], shape=(1, 7), dtype=int32)\n",
      "ROMEO: I must not speak,\n",
      "And do not scorn to me the could shall not say\n",
      "That shall be many than the manner to dischanged,\n",
      "And say I would so Marcius, now in that wind\n",
      "Than what will be made a vonced for me.\n",
      "\n",
      "JULIET:\n",
      "Ay, in the condemn'd with him on most surmand.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the enemies.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I will not be so dismand, and when I shall be so;\n",
      "The land and so revenge him a thousand country's hand,\n",
      "I will not think and pack'd him and look'd to his soul.\n",
      "\n",
      "LUCIO:\n",
      "What is the signior days shall be so.\n",
      "\n",
      "CAPULET:\n",
      "How now, my soul boy, What thou art not fould not so so.\n",
      "\n",
      "BRUTUS:\n",
      "Shall I be so.\n",
      "\n",
      "Second Keeper:\n",
      "What am money.\n",
      "\n",
      "SICINIUS:\n",
      "What had the waste of love, and in die to be some consul.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Here comes to come and wife.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Here's no servant, and how the light and more\n",
      "Shall be so done. I will make a desport?\n",
      "\n",
      "KING RICHARD III:\n",
      "So find many more hands, and depart mine.\n",
      "\n",
      "KING RICHARD III:\n",
      "Then shall be married to him a borthous more than the day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "\n",
    "def generate_text(model, start_string, temp):\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # converting start string to numbers\n",
    "  input_eval = [char_to_idx[s] for s in start_string]\n",
    "  print(input_eval)\n",
    "\n",
    "  # This operation is useful if you want to add a batch dimension to a single element. \n",
    "  # For example, if you have a single image of shape [height, width, channels], \n",
    "  # you can make it a batch of one image with expand_dims(image, 0), which will make the shape [1, height, width, channels].\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval)\n",
    "\n",
    "  # empty string to store results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting. (0-1)\n",
    "  temperature = temp\n",
    "\n",
    "  # here batch size = 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # remove batch dimentions\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temp= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "hwvhsoMXDCFU",
    "outputId": "9cbb13ea-5e69-4b30-f687-b9ed58442e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 27, 25, 17, 27, 10, 1]\n",
      "tf.Tensor([[30 27 25 17 27 10  1]], shape=(1, 7), dtype=int32)\n",
      "ROMEO: I will be so so.\n",
      "\n",
      "PETRUCHIO:\n",
      "What is the cause?\n",
      "\n",
      "CAPULET:\n",
      "What may come to the contract of his hands.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, then a fair of the company.\n",
      "\n",
      "KING RICHARD III:\n",
      "And what a son of some of your honour.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the condemn'd with a grave and the prince,\n",
      "And so did not so dispatch and soldier,\n",
      "And therefore the consul to the fair and disposed to him.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the manner of the warst of his father's life.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the consul, and the manner of him.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the state of his friends and the worst.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, then I say, the consul's days and more than the state,\n",
      "Which will be so strike a single and his son,\n",
      "And with him to him and the manner of the state,\n",
      "And so shall be so for a fair and all the people,\n",
      "And will I speak a courtery.\n",
      "\n",
      "KING RICHARD III:\n",
      "The consul of the contracted with him.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The worst senselves and the state of his son,\n",
      "Which have stand for his honour'd hands and \n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "\n",
    "def generate_text(model, start_string, temp):\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # converting start string to numbers\n",
    "  input_eval = [char_to_idx[s] for s in start_string]\n",
    "  print(input_eval)\n",
    "\n",
    "  # This operation is useful if you want to add a batch dimension to a single element. \n",
    "  # For example, if you have a single image of shape [height, width, channels], \n",
    "  # you can make it a batch of one image with expand_dims(image, 0), which will make the shape [1, height, width, channels].\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval)\n",
    "\n",
    "  # empty string to store results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting. (0-1)\n",
    "  temperature = temp\n",
    "\n",
    "  # here batch size = 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # remove batch dimentions\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temp= 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "id": "gFjbrGAJfQ7A",
    "outputId": "f977f6f5-57b6-472c-a0ce-8427708cb568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 27, 25, 17, 27, 10, 1]\n",
      "tf.Tensor([[30 27 25 17 27 10  1]], shape=(1, 7), dtype=int32)\n",
      "ROMEO: My willing speak?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "This was, ark not all and spirit,\n",
      "Bointy to complaive to unfortune fears, and highness\n",
      "Laster and many in God's upon me; or thy doth had hooned\n",
      "Hand the chimumed stands that saud within thy horring dither,\n",
      "Romeowh Farsio, my gentle Isage,\n",
      "With assurance things and divenjed madver himsal is any perfoced asperies?\n",
      "\n",
      "ISABELLA:\n",
      "Ko was you maidful well.\n",
      "\n",
      "KATHARINA:\n",
      "My triban, thou must have weld dead?\n",
      "What's ever loveds me to this spirit,\n",
      "No beghant Bulian:\n",
      "Which thou, damn'st none of what back.\n",
      "\n",
      "JULIET:\n",
      "Ay, jointed, my gifter Henry, the recharge man's noble father'd hours, and edemy too langurest young banished.\n",
      "And, in more soldier may sleep.\n",
      "\n",
      "ISABELLA:\n",
      "Poor man?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Romeo!\n",
      "I am aboved, or your success! and must never beautions,\n",
      "In Eagon!\n",
      "\n",
      "WARWICK:\n",
      "Grieve you, as my laughter.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What a haven baid!\n",
      "\n",
      "ANGELO:\n",
      "His hope, I must not fall wis truels too\n",
      "Sometimen's such a single or the end.\n",
      "\n",
      "WARWICK:\n",
      "And now now, beargon of him.\n",
      "\n",
      "ANG\n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "\n",
    "def generate_text(model, start_string, temp):\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # converting start string to numbers\n",
    "  input_eval = [char_to_idx[s] for s in start_string]\n",
    "  print(input_eval)\n",
    "\n",
    "  # This operation is useful if you want to add a batch dimension to a single element. \n",
    "  # For example, if you have a single image of shape [height, width, channels], \n",
    "  # you can make it a batch of one image with expand_dims(image, 0), which will make the shape [1, height, width, channels].\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval)\n",
    "\n",
    "  # empty string to store results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting. (0-1)\n",
    "  temperature = temp\n",
    "\n",
    "  # here batch size = 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # remove batch dimentions\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temp= 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFTIsUkcp2r0"
   },
   "source": [
    "Operations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".\n",
    "\n",
    "Trainable variables (created by tf.Variable or tf.compat.v1.get_variable, where trainable=True is default in both cases) are automatically watched. Tensors can be manually watched by invoking the watch method on this context manager.\n",
    "\n",
    "    tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "\n",
    "For example, consider the function y = x * x. The gradient at x = 3.0 can be computed as:\n",
    "\n",
    "    x = tf.constant(3.0)\n",
    "    with tf.GradientTape() as g:\n",
    "      g.watch(x)\n",
    "      y = x * x\n",
    "    dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "\n",
    "GradientTapes can be nested to compute higher-order derivatives. For example,\n",
    "\n",
    "    x = tf.constant(3.0)\n",
    "    with tf.GradientTape() as g:\n",
    "      g.watch(x)\n",
    "      with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "      dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
    "    d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n",
    "\n",
    "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:\n",
    "\n",
    "    x = tf.constant(3.0)\n",
    "    with tf.GradientTape(persistent=True) as g:\n",
    "      g.watch(x)\n",
    "      y = x * x\n",
    "      z = y * y\n",
    "    dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "    dy_dx = g.gradient(y, x)  # 6.0\n",
    "    del g  # Drop the reference to the tape\n",
    "\n",
    "By default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing watch_accessed_variables=False to the tape constructor:\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "      tape.watch(variable_a)\n",
    "      y = variable_a ** 2  # Gradients will be available for`variable_a`.\n",
    "      z = variable_b ** 3  # No gradients will be available since `variable_b` is not being watched.\n",
    "\n",
    "Note that when using models you should ensure that your variables exist when using watch_accessed_variables=False. Otherwise it's quite easy to make your first iteration not have any gradients:\n",
    "\n",
    "    a = tf.keras.layers.Dense(32)\n",
    "    b = tf.keras.layers.Dense(32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "      tape.watch(a.variables)  # Since `a.build` has not been called at this point `a.variables` will return an empty list and the tape will not be watching anything.\n",
    "      result = b(a(inputs))\n",
    "      tape.gradient(result, a.variables)  # The result of this computation will be a list of `None`s since a's variables are not being watched.\n",
    "\n",
    "Note that only tensors with real or complex dtypes are differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULYxVOh6HD7E"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YmijQBJg_5k"
   },
   "source": [
    "Contrast:\n",
    "\n",
    "    def dense(x, W, b):\n",
    "      return tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "    @tf.function\n",
    "    def multilayer_perceptron(x, w0, b0, w1, b1, w2, b2 ...):\n",
    "      x = dense(x, w0, b0)\n",
    "      x = dense(x, w1, b1)\n",
    "      x = dense(x, w2, b2)\n",
    "      ...\n",
    "      # You still have to manage w_i and b_i, and their shapes are\n",
    "        defined far away from the code.\n",
    "\n",
    "with the Keras version:\n",
    "\n",
    "    # Each layer can be called, with a signature equivalent to linear(x)\n",
    "    layers = [tf.keras.layers.Dense(hidden_size, activation=tf.nn.sigmoid) for _ in range(n)]                \n",
    "    perceptron = tf.keras.Sequential(layers)\n",
    "\n",
    "    # layers[3].trainable_variables => returns [w3, b3]\n",
    "    # perceptron.trainable_variables => returns [w0, b0, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6N0AaaDnp2A"
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape( ) as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(input_tensor= \n",
    "                          tf.keras.losses.sparse_categorical_crossentropy(y_true= target, \n",
    "                                                                          y_pred= predictions, \n",
    "                                                                            from_logits= True))\n",
    "  \n",
    "  # Keras models and layers offer the convenient 'variables' and 'trainable_variables' properties, \n",
    "  # which recursively gather up all dependent variables. This makes it easy to manage variables \n",
    "  # locally to where they are being used. (refer above)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ni1jH0nJNC4"
   },
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join('/content', \"ckpt_{epoch}\")\n",
    "\n",
    "model = model_build( vocab_size = vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ahvueBbnHt8m",
    "outputId": "b75e50d7-f15f-4c98-fecc-5b34bf0a2a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1754255294799805\n",
      "Epoch 1 Batch 100 Loss 2.288322687149048\n",
      "Epoch 1 Loss 2.0443\n",
      "Time taken for 1 epoch 11.471296787261963 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.9659547805786133\n",
      "Epoch 2 Batch 100 Loss 1.820554494857788\n",
      "Epoch 2 Loss 1.7222\n",
      "Time taken for 1 epoch 10.026393413543701 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.669248104095459\n",
      "Epoch 3 Batch 100 Loss 1.625135898590088\n",
      "Epoch 3 Loss 1.5390\n",
      "Time taken for 1 epoch 10.032483577728271 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5384776592254639\n",
      "Epoch 4 Batch 100 Loss 1.4924185276031494\n",
      "Epoch 4 Loss 1.4415\n",
      "Time taken for 1 epoch 9.897800922393799 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.421339988708496\n",
      "Epoch 5 Batch 100 Loss 1.4078408479690552\n",
      "Epoch 5 Loss 1.3751\n",
      "Time taken for 1 epoch 10.052716255187988 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3726691007614136\n",
      "Epoch 6 Batch 100 Loss 1.3876155614852905\n",
      "Epoch 6 Loss 1.3272\n",
      "Time taken for 1 epoch 10.050699472427368 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2765899896621704\n",
      "Epoch 7 Batch 100 Loss 1.338740587234497\n",
      "Epoch 7 Loss 1.3208\n",
      "Time taken for 1 epoch 10.013462781906128 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2312674522399902\n",
      "Epoch 8 Batch 100 Loss 1.3026504516601562\n",
      "Epoch 8 Loss 1.2785\n",
      "Time taken for 1 epoch 10.017131328582764 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1907991170883179\n",
      "Epoch 9 Batch 100 Loss 1.2574515342712402\n",
      "Epoch 9 Loss 1.2876\n",
      "Time taken for 1 epoch 10.022722721099854 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1921272277832031\n",
      "Epoch 10 Batch 100 Loss 1.239667296409607\n",
      "Epoch 10 Loss 1.2474\n",
      "Time taken for 1 epoch 10.094823837280273 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.1304038763046265\n",
      "Epoch 11 Batch 100 Loss 1.1984503269195557\n",
      "Epoch 11 Loss 1.1991\n",
      "Time taken for 1 epoch 9.984236240386963 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0817675590515137\n",
      "Epoch 12 Batch 100 Loss 1.1611994504928589\n",
      "Epoch 12 Loss 1.1701\n",
      "Time taken for 1 epoch 10.086061000823975 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.0644326210021973\n",
      "Epoch 13 Batch 100 Loss 1.1253539323806763\n",
      "Epoch 13 Loss 1.1568\n",
      "Time taken for 1 epoch 10.098074436187744 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.0325512886047363\n",
      "Epoch 14 Batch 100 Loss 1.0482486486434937\n",
      "Epoch 14 Loss 1.0914\n",
      "Time taken for 1 epoch 10.088799476623535 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.9930705428123474\n",
      "Epoch 15 Batch 100 Loss 1.0514111518859863\n",
      "Epoch 15 Loss 1.0678\n",
      "Time taken for 1 epoch 10.078505992889404 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.9693559408187866\n",
      "Epoch 16 Batch 100 Loss 0.9628669619560242\n",
      "Epoch 16 Loss 1.0311\n",
      "Time taken for 1 epoch 9.989897727966309 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.897329568862915\n",
      "Epoch 17 Batch 100 Loss 0.9938720464706421\n",
      "Epoch 17 Loss 0.9999\n",
      "Time taken for 1 epoch 9.990666389465332 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.8676051497459412\n",
      "Epoch 18 Batch 100 Loss 0.9251348376274109\n",
      "Epoch 18 Loss 0.9483\n",
      "Time taken for 1 epoch 10.02344274520874 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.8076867461204529\n",
      "Epoch 19 Batch 100 Loss 0.8923636078834534\n",
      "Epoch 19 Loss 0.9300\n",
      "Time taken for 1 epoch 10.014105558395386 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.7986055612564087\n",
      "Epoch 20 Batch 100 Loss 0.866606593132019\n",
      "Epoch 20 Loss 0.8599\n",
      "Time taken for 1 epoch 9.996323823928833 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.736359179019928\n",
      "Epoch 21 Batch 100 Loss 0.7974749803543091\n",
      "Epoch 21 Loss 0.8622\n",
      "Time taken for 1 epoch 9.956365585327148 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.7135876417160034\n",
      "Epoch 22 Batch 100 Loss 0.7906237840652466\n",
      "Epoch 22 Loss 0.8438\n",
      "Time taken for 1 epoch 9.98558759689331 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.6805621385574341\n",
      "Epoch 23 Batch 100 Loss 0.779574990272522\n",
      "Epoch 23 Loss 0.7950\n",
      "Time taken for 1 epoch 10.117630004882812 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.6632631421089172\n",
      "Epoch 24 Batch 100 Loss 0.7230767607688904\n",
      "Epoch 24 Loss 0.7571\n",
      "Time taken for 1 epoch 10.00714898109436 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.5955040454864502\n",
      "Epoch 25 Batch 100 Loss 0.71584153175354\n",
      "Epoch 25 Loss 0.7389\n",
      "Time taken for 1 epoch 10.07948637008667 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.5960854887962341\n",
      "Epoch 26 Batch 100 Loss 0.6746395826339722\n",
      "Epoch 26 Loss 0.6922\n",
      "Time taken for 1 epoch 10.128275871276855 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.5712471008300781\n",
      "Epoch 27 Batch 100 Loss 0.6652942895889282\n",
      "Epoch 27 Loss 0.6718\n",
      "Time taken for 1 epoch 10.076830625534058 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.5400032997131348\n",
      "Epoch 28 Batch 100 Loss 0.6365725994110107\n",
      "Epoch 28 Loss 0.6869\n",
      "Time taken for 1 epoch 10.057198762893677 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.5070273876190186\n",
      "Epoch 29 Batch 100 Loss 0.6035774946212769\n",
      "Epoch 29 Loss 0.6360\n",
      "Time taken for 1 epoch 10.005269527435303 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.465404748916626\n",
      "Epoch 30 Batch 100 Loss 0.5846977829933167\n",
      "Epoch 30 Loss 0.6359\n",
      "Time taken for 1 epoch 10.06916069984436 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "  start= time.time()\n",
    "\n",
    "  # initializing the hidden state at the start of every epoch\n",
    "  # initally hidden is None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for(i, (ip, op)) in enumerate(dataset):\n",
    "    loss = train_step(inp = ip, target = op)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {}'.format(epoch+1, i, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "B5Illz35itYM",
    "outputId": "477a31b8-a873-4885-d0e3-60f1a0ece2bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_build(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights('/content/ckpt_29')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "4b8-_4kVkMva",
    "outputId": "890c6c99-e6e3-4459-904e-06af2b371f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 27, 25, 17, 27, 10, 1]\n",
      "tf.Tensor([[30 27 25 17 27 10  1]], shape=(1, 7), dtype=int32)\n",
      "ROMEO: here is that heaven\n",
      "from the foul musician, and the Lord Hastings,\n",
      "Her four grace with his life to do him doing of the king.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Northumberland comes back to die:\n",
      "I will take order for her beauty makes\n",
      "As doth a sail, fill'd with a stamp, of which he hath wronged on\n",
      "my heart.\n",
      "\n",
      "ANTONIO:\n",
      "Nay, good my lord,\n",
      "I dare deliver me the matter: then,\n",
      "if the in the other's tale against our soldiers?\n",
      "\n",
      "WARWICK:\n",
      "Henry made you now?\n",
      "The way or wife?\n",
      "\n",
      "ELBOW:\n",
      "My Lord of Surrey, why for a name and the match'd that she say,\n",
      "I was too hot a day as her power in his hands, the aughter of Lancaster.\n",
      "You are a louthed splay'd for a new-made grave\n",
      "And hope the love to her her; thou shalt not have accuseved\n",
      "To some remote and covert the like a clout\n",
      "Attended to by favour it hath set adied before the watch, and that it may put before the watch, and cruel with the fire\n",
      "Of his bosom of the maid you are,\n",
      "That it may bear me speak, I'll be your honour,\n",
      "Doing to behind the heavens that hath done me so,\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "\n",
    "def generate_text(model, start_string, temp):\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # converting start string to numbers\n",
    "  input_eval = [char_to_idx[s] for s in start_string]\n",
    "  print(input_eval)\n",
    "\n",
    "  # This operation is useful if you want to add a batch dimension to a single element. \n",
    "  # For example, if you have a single image of shape [height, width, channels], \n",
    "  # you can make it a batch of one image with expand_dims(image, 0), which will make the shape [1, height, width, channels].\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval)\n",
    "\n",
    "  # empty string to store results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting. (0-1)\n",
    "  temperature = temp\n",
    "\n",
    "  # here batch size = 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # remove batch dimentions\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temp= 0.5))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Shakespeare_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
